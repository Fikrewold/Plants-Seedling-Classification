{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "847ca56a-29e7-4571-9960-fa6a70038213",
    "_uuid": "182dce896e280e8525a752ced6249dbb4411f69c"
   },
   "source": [
    "# **Plant Seedlings Classification CNN**\n",
    " \n",
    " **By: Neil Shah**\n",
    "\n",
    "\n",
    "## **Introduction**<br>\n",
    "This is an 8-layer convolutional neural network model for recognizing 12 different plant seedlings from images at various stages of growth. I used Keras, a high-level neural networks library, with the TensorFlow backend. I achieved 92.695% accuracy in 5 hours of training for 75 epochs. I trained my CNN on Jupyter Notebook and loaded the model by creating a new dataset. I am still making changes to my model to improve the accuracy. The notebook has 3 main parts:\n",
    "* Data  Preparation\n",
    "* CNN model and evaluation\n",
    "* Predictions and Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import os as os\n",
    "from PIL import Image\n",
    "from array import array\n",
    "import cv2 as cv2\n",
    "from glob import glob\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "seed = 7\n",
    "scale=70\n",
    "np.random.seed(seed)\n",
    "\n",
    "# for API\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Concatenate, Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "#K.set_image_dim_ordering( 'tf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/atq765/My Drive/PG-AIML/Computer Vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen',\n",
    "          'Loose Silky-bent', 'Maize','Scentless Mayweed', 'Shepherds Purse',\n",
    "          'Small-flowered Cranesbill', 'Sugar beet']\n",
    "data_dir = 'C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cwd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcwd\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cwd' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/train\\\\Black-grass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [83]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m species_id, sp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(species):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      5\u001b[0m         train_data\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sp, file), species_id, sp])\n\u001b[0;32m      7\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeciesId\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecies\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/train\\\\Black-grass'"
     ]
    }
   ],
   "source": [
    "# Organize training files into DataFrame\n",
    "train_data = []\n",
    "for species_id, sp in enumerate(species):\n",
    "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
    "        train_data.append(['train/{}/{}'.format(sp, file), species_id, sp])\n",
    "        \n",
    "train = pd.DataFrame(train_data, columns=['File', 'SpeciesId','Species'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize training files into DataFrame\n",
    "train_data = []\n",
    "for species_id, sp in enumerate(species):\n",
    "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
    "        train_data.append(['train/{}/{}'.format(sp, file), species_id, sp])\n",
    "        \n",
    "train = pd.DataFrame(train_data, columns=['File', 'SpeciesId','Species'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is 1016-95DA\n",
      "\n",
      " Directory of C:\\Users\\atq765\\My Drive\\PG-AIML\\Computer Vision\\plant-seedlings-classification\n",
      "\n",
      "07/11/2022  10:02 AM    <DIR>          .\n",
      "07/11/2022  10:02 AM    <DIR>          ..\n",
      "07/09/2022  10:52 PM       233,472,128 images.npy\n",
      "07/09/2022  10:52 PM            68,874 Labels.csv\n",
      "07/11/2022  01:11 AM        13,368,912 my_model.hdf5\n",
      "07/11/2022  10:02 AM    <DIR>          test\n",
      "07/11/2022  10:01 AM    <DIR>          train\n",
      "               3 File(s)    246,909,914 bytes\n",
      "               4 Dir(s)  245,723,230,208 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls \"C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToTrain = \"C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/train\"\n",
    "testPath = \"C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5dd0c0ea-b782-452b-b65f-64bd6491222c",
    "_uuid": "11fee9b79b2d9aea38e25db8a047c93f53a705e2"
   },
   "source": [
    "## **Data Preparation**\n",
    "### **Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\envs\\time_series\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\programdata\\anaconda3\\envs\\time_series\\lib\\site-packages (from opencv-python) (1.21.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images = \"C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/pny\"\n",
    "images = glob(path_to_images)\n",
    "trainingset = []\n",
    "traininglabels = []\n",
    "num = len(images)\n",
    "count = 1\n",
    "#READING IMAGES AND RESIZING THEM\n",
    "for i in images:\n",
    "    print(str(count)+'/'+str(num),end='r')\n",
    "    trainingset.append(cv2.resize(cv2.imread(i),(scale,scale)))\n",
    "    traininglabels.append(i.split('/')[-2])\n",
    "    count=count+1\n",
    "trainingset = np.asarray(trainingset)\n",
    "traininglabels = pd.DataFrame(traininglabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen',\n",
    "          'Loose Silky-bent', 'Maize','Scentless Mayweed', 'Shepherds Purse',\n",
    "          'Small-flowered Cranesbill', 'Sugar beet']\n",
    "data_dir = '../input/plant-seedlings-classification/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m     28\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mnew_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_train = []\n",
    "sets = []; getEx = True\n",
    "for i in trainingset:\n",
    "    blurr = cv2.GaussianBlur(i,(5,5),0)\n",
    "    hsv = cv2.cvtColor(blurr,cv2.COLOR_BGR2HSV)\n",
    "    #GREEN PARAMETERS\n",
    "    lower = (25,40,50)\n",
    "    upper = (75,255,255)\n",
    "    mask = cv2.inRange(hsv,lower,upper)\n",
    "    struc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(11,11))\n",
    "    mask = cv2.morphologyEx(mask,cv2.MORPH_CLOSE,struc)\n",
    "    boolean = mask>0\n",
    "    new = np.zeros_like(i,np.uint8)\n",
    "    new[boolean] = i[boolean]\n",
    "    new_train.append(new)\n",
    "    if getEx:\n",
    "        plt.subplot(2,3,1);plt.imshow(i) # ORIGINAL\n",
    "        plt.subplot(2,3,2);plt.imshow(blurr) # BLURRED\n",
    "        plt.subplot(2,3,3);plt.imshow(hsv) # HSV CONVERTED\n",
    "        plt.subplot(2,3,4);plt.imshow(mask) # MASKED\n",
    "        plt.subplot(2,3,5);plt.imshow(boolean) # BOOLEAN MASKED\n",
    "        plt.subplot(2,3,6);plt.imshow(new) # NEW PROCESSED IMAGE\n",
    "        plt.show()\n",
    "        getEx = False\n",
    "new_train = np.asarray(new_train)\n",
    "# CLEANED IMAGES\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.imshow(new_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "images\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     trainImagesPaths\u001b[38;5;241m.\u001b[39mappend(imgPath) \u001b[38;5;66;03m# paths to images\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     trainImagesCategories\u001b[38;5;241m.\u001b[39mappend(imgFolder) \u001b[38;5;66;03m# labels\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     trainImg\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgPath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m71\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m71\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;66;03m# images\u001b[39;00m\n\u001b[0;32m     25\u001b[0m t1\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(t1\u001b[38;5;241m-\u001b[39mt0,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "listing = os.listdir( pathToTrain ) \n",
    "num_folders = len(listing)\n",
    "print ( num_folders)\n",
    "trainArray = [[]]\n",
    "trainImagesPaths = []\n",
    "trainImagesCategories = []\n",
    "trainImg = []\n",
    "scaleTo = 71\n",
    "seed = 7\n",
    "\n",
    "t0=time.time()\n",
    "\n",
    "for imgFolder in listing:\n",
    "  print(imgFolder)\n",
    "  path = \"C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/train/\"+\"images\"+ \"/\"\n",
    "  files = os.listdir( path ) \n",
    "  for imgFile in files:\n",
    "    imgPath =cv2.imread(path+\"images.npy\")\n",
    "    trainArray.append([imgPath, imgFolder]) # image path, image folder\n",
    "    trainImagesPaths.append(imgPath) # paths to images\n",
    "    trainImagesCategories.append(imgFolder) # labels\n",
    "    \n",
    "    trainImg.append(cv2.resize(cv2.imread(imgPath), (71,71))) # images\n",
    "    \n",
    "t1=time.time()\n",
    "print(t1-t0,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = os.listdir( pathToTrain ) \n",
    "num_folders = len(listing)\n",
    "print ( num_folders)\n",
    "trainArray = [[]]\n",
    "trainImagesPaths = []\n",
    "trainImagesCategories = []\n",
    "trainImg = []\n",
    "scaleTo = 71\n",
    "seed = 7\n",
    "\n",
    "t0=time.time()\n",
    "\n",
    "for imgFolder in listing:\n",
    "  print(imgFolder)\n",
    "  path = \"/content/drive/My Drive/plants/train/\" + imgFolder + '/'\n",
    "  files = os.listdir( path ) \n",
    "  for imgFile in files:\n",
    "    imgPath = path + imgFile\n",
    "    trainArray.append([imgPath, imgFolder]) # image path, image folder\n",
    "    trainImagesPaths.append(imgPath) # paths to images\n",
    "    trainImagesCategories.append(imgFolder) # labels\n",
    "    \n",
    "    trainImg.append(cv2.resize(cv2.imread(imgPath), (scaleTo, scaleTo))) # images\n",
    "    \n",
    "t1=time.time()\n",
    "print(t1-t0,\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpath\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainImagesPaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "trainImagesPaths[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen',\n",
    "          'Loose Silky-bent', 'Maize','Scentless Mayweed', 'Shepherds Purse',\n",
    "          'Small-flowered Cranesbill', 'Sugar beet']\n",
    "data_dir = 'C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Black-grass',\n",
       " 'Charlock',\n",
       " 'Cleavers',\n",
       " 'Common Chickweed',\n",
       " 'Common wheat',\n",
       " 'Fat Hen',\n",
       " 'Loose Silky-bent',\n",
       " 'Maize',\n",
       " 'Scentless Mayweed',\n",
       " 'Shepherds Purse',\n",
       " 'Small-flowered Cranesbill',\n",
       " 'Sugar beet']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3984238628.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [33]\u001b[1;36m\u001b[0m\n\u001b[1;33m    x, y =[species_id, sp for species_id, sp in enumerate(species)]\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "0bb76c73-a77b-42b3-b5ec-c8f47f30c875",
    "_uuid": "4e6da99e10a61bbdd5151bbe76b444263343391b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/train\\\\Black-grass'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m species_id, sp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(species):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      5\u001b[0m         train_data\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(sp, file), species_id, sp])\n\u001b[0;32m      7\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(train_data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpeciesId\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpecies\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/atq765/My Drive/PG-AIML/Computer Vision/plant-seedlings-classification/train\\\\Black-grass'"
     ]
    }
   ],
   "source": [
    "# Organize training files into DataFrame\n",
    "train_data = []\n",
    "for species_id, sp in enumerate(species):\n",
    "    for file in os.listdir(os.path.join(train_dir, sp)):\n",
    "        train_data.append(['train/{}/{}'.format(sp, file), species_id, sp])\n",
    "        \n",
    "train = pd.DataFrame(train_data, columns=['File', 'SpeciesId','Species'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "29c282ca-f26c-4964-ab19-62f6cd8b3614",
    "_uuid": "b58ad4b8a8c3f09577d41f68022486456edf7a2c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Randomize the order of training set\u001b[39;00m\n\u001b[0;32m      2\u001b[0m SEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m----> 3\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED) \n\u001b[0;32m      4\u001b[0m train\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(train)) \u001b[38;5;66;03m# Reset indices\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Randomize the order of training set\n",
    "SEED = 42\n",
    "train = train.sample(frac=1, random_state=SEED) \n",
    "train.index = np.arange(len(train)) # Reset indices\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b153d157-ed76-4508-9c2b-4fd07ca58a8c",
    "_uuid": "37a488464f0a6c9cc23fda802df2900b9d412d03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot a histogram\n",
    "plt.hist(train['SpeciesId'])\n",
    "plt.title('Frequency Histogram of Species')\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5b470ac8-edaf-480b-a4ad-9e2c2ea100ff",
    "_uuid": "4c6984bf16aeb97c2ca68470aa34c9b4b9d9ec4a"
   },
   "source": [
    "The histogram shows that there is a large range in the number of training samples for each of the species. If we decided to stratify the training set by evenly distributing the number of samples per species (~ 200), then we would be omitting a large amount of potentially useful training data. For that reason, I won't perform stratification, but it's something to consider for future improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "245c3e7b-129b-4358-91a8-0046321701bf",
    "_uuid": "8becd8e2045f322ca25d7623ff19e29b05f542b8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Organize test files into DataFrame\n",
    "test_data = []\n",
    "for file in os.listdir(test_dir):\n",
    "    test_data.append(['test/{}'.format(file), file])\n",
    "test = pd.DataFrame(test_data, columns=['Filepath', 'File'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "254ca8d8-e02a-44b9-ac65-ba4e7bba9990",
    "_uuid": "8c163cfa5d8345047077b29dbb37756290bab677"
   },
   "source": [
    "### **Plot training images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77923d06-f45b-434f-a200-519ded3f2fa8",
    "_uuid": "a091d023a8292d5c1e07c4033f07dfa8855fb6b4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display images for different species\n",
    "def plot_species(species, rows, cols):\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "    species_files = train['File'][train['Species'] == species].values\n",
    "    n = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            image_path = os.path.join(data_dir, species_files[n])\n",
    "            ax[i, j].set_xticks([])\n",
    "            ax[i, j].set_yticks([])\n",
    "            ax[i, j].imshow(cv2.imread(image_path))\n",
    "            n += 1\n",
    "# Displays first n images of class from training set\n",
    "plot_species('Black-grass', 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "97a84260-d1bb-4f53-b326-c00d9a402b07",
    "_uuid": "0bff87148341e1d01468e6bc46f9614b777e2679"
   },
   "source": [
    "### **Image preprocessing**\n",
    "I must extend credit to Gábor Vecsei's [Plant Seedlings Fun with Computer Vision](https://www.kaggle.com/gaborvecsei/plant-seedlings-fun-with-computer-vision) for helping me to understand and implement image preprocessing techniques. I wrote functions for reading a BGR image and resizing the image. For resizing the image, I used the INTER_AREA interpolation method, which resamples using pixel area relation. It's the preferred interpolation method for image decimation because it provides moiré-free (non-wavy) results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7ed1f74f-803f-48a3-af71-8872d62d1475",
    "_uuid": "563a0196c6c63ad9b99d88d6de141638c0ffefd6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 66\n",
    "\n",
    "def read_image(filepath):\n",
    "    return cv2.imread(os.path.join(data_dir, filepath)) # Loading a color image is the default flag\n",
    "# Resize image to target size\n",
    "def resize_image(image, image_size):\n",
    "    return cv2.resize(image.copy(), image_size, interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0144ed8e-9b11-4c2f-b16c-41258faebf63",
    "_uuid": "132955c330cccf9f3688f899414fdf84a29a6bcf"
   },
   "source": [
    "There are many different types of image segmentation. I will use the most basic type called thresholding. It is a non-linear operation converts an image into a binary images where the two levels are assigned pixels based on whether they're above or below the specified threshold value. <br><br>\n",
    "First, I will convert from the BGR to the HSV color-space which will be useful for extracting green-colored objects. In HSV, the hue of a color refers to the pure color it resembles and the value represents the brightness. The saturation describes the shade of the color, such as pink and salmon representing different types of the red. \n",
    "![](http://www.nmt.edu/tcc/help/pubs/colortheory/img/cone.png)\n",
    "Source: http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html<br>\n",
    "\n",
    "After making the color-space conversion, we threshold the HSV image for a range of green color. Another technique we can apply is a morphological transformation. I used the closing transformation because it's useful for closing small holes in the objects. In doing this, we have to specify a structural element or kernel which slides through the image applying transformations on the specified window size. <br><br>\n",
    "Finally, I extract the green objects alone by performing a bitwise-AND operation between the mask and the original image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9efc1017-2952-4661-937f-5a6dd1f3dfc8",
    "_uuid": "d8ba71ecfdff4e498171500562f9e907d2dd8681",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image segmentation\n",
    "def create_mask(image):\n",
    "    # Convert from BGR to HSV color-space to extract colored object\n",
    "    image_hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    # Define range of green in HSV\n",
    "    lower_green = np.array([30, 100, 50])\n",
    "    upper_green = np.array([85, 255, 255])\n",
    "    # Threshold the HSV image to get only green colors\n",
    "    mask = cv2.inRange(image_hsv, lower_green, upper_green)\n",
    "    # We will use a morphological operation called closing to close small holes in the image\n",
    "    # We need a kernel or structuring element to determine the nature of the operation\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "    return mask\n",
    "\n",
    "def segment_image(image):\n",
    "    mask = create_mask(image)\n",
    "    res = cv2.bitwise_and(image, image, mask=mask) # Bitwise-AND mask and original image\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "deca7a09-7710-463f-a62a-6a8d3369f6a9",
    "_uuid": "fbca3e1f8accf0faa475fa657106f9c324df28f6"
   },
   "source": [
    "### **Plot segmented images**<br>\n",
    "We can show a side-by-side comparison of the original, masked, segmented, and resized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "386aed5f-d065-402f-8fc7-073aa068fb47",
    "_uuid": "3b5158fae1a81fde87c059e0dcdce2e4baec1599",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_segmented_images(species, n):\n",
    "    fig, ax = plt.subplots(n, 4, figsize=(20, 20))\n",
    "    species_files = train['File'][train['Species'] == species].values\n",
    "    for i in range(n):\n",
    "        image = read_image(species_files[i])\n",
    "        image_masked = create_mask(image)\n",
    "        image_segmented = segment_image(image)\n",
    "        image_resized = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        ax[i, 0].set_axis_off()\n",
    "        ax[i, 1].set_axis_off()\n",
    "        ax[i, 2].set_axis_off()\n",
    "        ax[i, 3].set_axis_off()\n",
    "        ax[i, 0].imshow(image)\n",
    "        ax[i, 1].imshow(image_masked)\n",
    "        ax[i, 2].imshow(image_segmented)\n",
    "        ax[i, 3].imshow(image_resized)\n",
    "# Displays first n images of class from training set\n",
    "show_segmented_images('Maize', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1d5042fa-41d1-4f2a-ac9d-3e74af2c3a8e",
    "_uuid": "dd527fb43de42f8d94e244058c52d843bb055cd0"
   },
   "source": [
    "### **Extract features**\n",
    "Since the image sizes vary, I reshaped all of the images to 66x66x3. Keras requires an extra dimension which corresponds to the channels. For RGB images, there are 3 channels. I also performed normalization to help the CNN converge faster. Note that we resize the image after segmentation to reduce the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ac626ec-c42a-4ef5-b12d-27c2c2a8432d",
    "_uuid": "e136f147936f1cd42ebc2c2e29dcc0d32fb9ab4a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.zeros((train.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for i, file in tqdm(enumerate(train['File'].values)):\n",
    "    image = read_image(file)\n",
    "    image_segmented = segment_image(image)\n",
    "    X_train[i] = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "# Normalize the data\n",
    "X_train = X_train / 255.\n",
    "print('Train Shape: {}'.format(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a7316192-cf21-4a68-89e7-5f3bd6d77879",
    "_uuid": "6417ef603244f72bc881e034ff314a07c2a630af"
   },
   "source": [
    "### **Label encoding**\n",
    "We encode the labels to one-hot vectors. The integer encoding (0-11) is removed and a new binary value is added for each integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "30b0ef43-1571-4cdb-b4e4-20faf309b2a7",
    "_uuid": "ac3200e347197bab5290b1ecc2338f19fff38206",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = train['SpeciesId'].values\n",
    "Y_train = to_categorical(Y_train, num_classes=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "297fb357-088d-4694-980f-02eb84bffb96",
    "_uuid": "191354375100f3c15030c6474b916d90e540d21b"
   },
   "source": [
    "### **Split training and validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "11dfcc96-212c-4783-a9b0-44e269b77ee0",
    "_uuid": "c43e9ad52355cf4a3dcaabef7db807f11e47b3ee",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 75\n",
    "\n",
    "# Split the train and validation sets \n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c14820ec-d7c6-41ea-ae19-a3ec4637a4d7",
    "_uuid": "a3e403bf3c5081a191dffda31c6b8e1d6f1c0a16"
   },
   "source": [
    "I chose to split 90% of the data into a training group and the remaining 10% into a validation group for evaluating the model's performance.<br><br><br>\n",
    "\n",
    "We can get a better sense of the training samples by looking at some images with the known labels which are shown in the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5a57955e-9b3f-4bdf-b44f-1a490234415a",
    "_uuid": "dd1f66cff312e07b6e7a54223d5131cd1b869e6d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(15, 15))\n",
    "for i in range(4):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(X_train[i])\n",
    "    ax[i].set_title(species[np.argmax(Y_train[i])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dacce5f6-9b44-4a00-86ba-24eaaa4ff677",
    "_uuid": "af90c65b688d2842499666dd0d3b1384246b52e8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b1328e0-e46c-4e85-ac9d-dba586bd4c42",
    "_uuid": "527c7195b9f68f6dda7fa54e4ea2600a2add20c3"
   },
   "source": [
    "## **CNN model and evaluation**<br>\n",
    "\n",
    "### **Model Architecture**\n",
    "The Sequential model is a linear stack of layers. The first layer in the model needs to receive information about its input shape and the following layers will do automatic shape reference. The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters.  During the forward pass, each filter is convolved across the image to produce a 2D activation map of each filter. Then stacking the activation maps for each filter forms the full output of the convolutional layer. The Batch normalization layer normalizes the activations of the convolutional layers by applying a transformation that maintains the mean close to 0 and the standard deviation close to 1.  The max pooling layer serves as a form of non-linear downsampling. In this case, the 2x2 filters compute the maximum value of four pixels and make a stride of 2 pixels (width and height) at each depth. There are a total of 6 convolutional layers and 3 max pooling layers. The flatten layer is used to convert the final feature map into a 1D vector, combining all of the features of the previous layer. In the final layer, I used softmax activation so the neural network outputs the probability distribution for each class.\n",
    "\n",
    "For the convolutional and dense layers, I used the ReLU activation function. For training deep neural networks, ReLU is more effective than the sigmoid and tangent activation functions because it prevents gradients from saturating. The vanishing gradient problem causes the neural network to get stuck preventing meaningful learning from taking place.\n",
    "\n",
    "Before training the model, we need to configure the learning process by specifying the optimizer, loss function, and list of metrics. The loss function measures the error rate between the model's predicted and observed labels. The categorical crossentropy loss function is computed by taking the average of all cross-entropies in the sample. It will measure the probability that the training sample belongs to an individual class. The cost function is the average of the loss function over a large number of training samples. The goal of the optimization algorithm is to minimize the cost function by iteratively updating the weights and biases. I used the Adam (short for Adaptive Moment Estimation) optimizer because it's effective and achieves good results quickly. For information about how the optimizer works, [click here](https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38a915ba-fed4-4870-80f2-f3594e85a9a3",
    "_uuid": "1cb9280f2cd8cb84a5be23f3f75844ed4331f919",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construct_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "                activation='relu'))\n",
    "    model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch\n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), strides=(1, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(Flatten()) # Flatten the input\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "    # Configure the learning process\n",
    "    # The loss function is the objective that the model will try to minimize\n",
    "    # For any classification problem, use accuracy metric\n",
    "    optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1248cf67-7538-4da3-915e-ded27cd7b284",
    "_uuid": "2d42fb0f010398cc13284025b72b46b4eb9772b9"
   },
   "source": [
    "### **Training the CNN**\n",
    "\n",
    "ReduceLROnPlateau reduces the learning rate when the validation accuracy has stopped improving. Models often benefit from reducing the learning rate by a factor of 2 to 10 once learning stagnates. My annealer reduces the learning rate by 50% when the validation accuracy hasn't increased for 5 epochs. Reducing the learning rate allows the optimizer to take smaller steps to reach the global minimum, which increases the rate of convergence. \n",
    "\n",
    "The ImageDataGenerator is a data augmentation technique that works by applying small transformations to the training samples to generate additional data. This makes the existing training dataset larger. For example, the training images may be rotated by a certain number of degrees or zoomed in by a small percentage. There can also be random horizontal and vertical shifts, ZCA whitening, mean and standard deviation normalization, etc. \n",
    "\n",
    "I trained the model for 75 epochs, although the training and validation accuracies only made miniscule improvements after 55 epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "68e1ecbf-6772-4d9c-b9dc-f9509607ee23",
    "_uuid": "d60cf1d2fd16557b4af1a50c463570e86b31bfea",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    model = construct_model()\n",
    "    annealer = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=5, verbose=1, min_lr=1e-5)\n",
    "    checkpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "    # Generates batches of image data with data augmentation\n",
    "    datagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n",
    "                            width_shift_range=0.2, # Range for random horizontal shifts\n",
    "                            height_shift_range=0.2, # Range for random vertical shifts\n",
    "                            zoom_range=0.2, # Range for random zoom\n",
    "                            horizontal_flip=True, # Randomly flip inputs horizontally\n",
    "                            vertical_flip=True) # Randomly flip inputs vertically\n",
    "    \n",
    "    datagen.fit(X_train)\n",
    "    # Fits the model on batches with real-time data augmentation\n",
    "    hist = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "                   steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "                   epochs=EPOCHS,\n",
    "                   verbose=2,\n",
    "                   callbacks=[annealer, checkpoint],\n",
    "                   validation_data=(X_val, Y_val))\n",
    "# train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "544cbd02-df73-407f-b9c9-41f18cbec207",
    "_uuid": "acd92c55fceb75e8a79bf4acdde95771971e0507"
   },
   "source": [
    "### **Model evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "04f2baf0-3d11-4342-8baa-ffd11ea661a3",
    "_uuid": "c2eeaeb2b0223805287204924ea7fcb85f4b2e02",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = load_model('../input/plant-seedling-models/model.h5')\n",
    "final_loss, final_accuracy = final_model.evaluate(X_val, Y_val)\n",
    "print('Final Loss: {}, Final Accuracy: {}'.format(final_loss, final_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d68a6840-2c16-44c5-bd95-f40f80b034db",
    "_uuid": "24a1fe18d19dc5cc0173462db478203abd8c29eb"
   },
   "source": [
    "I plotted a confusion matrix. It seems as though the CNN has trouble distinguishing between  black-grass and loose silky-bent images. These species look quite similar to each other so its easy to understand why the CNN is making errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ce2f2b4-ec22-4e5e-a614-1381645300a4",
    "_uuid": "e49682a5f2f101d8d9afa30ac3bdff2b39b3ec87",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = final_model.predict(X_val)\n",
    "\n",
    "Y_pred = np.argmax(Y_pred, axis=1)\n",
    "Y_true = np.argmax(Y_val, axis=1)\n",
    "\n",
    "cm = confusion_matrix(Y_true, Y_pred)\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = sns.heatmap(cm, cmap=plt.cm.Greens, annot=True, square=True, xticklabels=species, yticklabels=species)\n",
    "ax.set_ylabel('Actual', fontsize=40)\n",
    "ax.set_xlabel('Predicted', fontsize=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2be72549-94af-4c83-b072-3c3586cf36ed",
    "_uuid": "b2f4f4c42cde4040140b2a690c0f383156cd295f"
   },
   "source": [
    "## **Submit Predictions**\n",
    "<br>\n",
    "Extracts the testing features and makes predictions. The results are saved in the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "85de3462-82a1-4007-b5bf-84e1feee32df",
    "_uuid": "8515565afa295022a545cf84afd1e778329b41a8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.zeros((test.shape[0], IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "for i, file in tqdm(enumerate(test['Filepath'].values)):\n",
    "    image = read_image(file)\n",
    "    image_segmented = segment_image(image)\n",
    "    X_test[i] = resize_image(image_segmented, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d352c262-6a0d-4ebd-a68e-3b317e063792",
    "_uuid": "47eda25d177d577075c7046c39b97bdf828e312b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = final_model.predict(X_test)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "df = pd.DataFrame({'file': [file for file in test['File'].values], 'species': [species[i] for i in predictions]})\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
